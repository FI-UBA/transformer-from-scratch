---
# The dataset to use, in this case, the OPUS Books dataset from Hugging Face that
# will be downloaded and cached in the local machine.
dataset:
  # Datasource name
  name: "opus_books"
  # The source language
  src_lang: "es"
  # The target language
  tgt_lang: "pt"
  # Filter from the dataset the sentences exceding seq_len-2 tokens (long sentences)
  filter_long_sentences: true

# The model configuration
model:
  # Maximum sequence length
  seq_len: 102
  # Dimentionality of the embeddings
  d_model: 512
  # Number of heads in the multi-head attention
  num_heads: 4
  # Number of encoder layers
  num_encoder_layers: 2
  # Number of decoder layers
  num_decoder_layers: 2
  # The feedforward dimensionality for the last fully connected layer
  dim_feedforward: 2048
  # The dropout rate
  dropout: 0.1

# The training configuration
training:
  # The batch size
  batch_size: 32
  # The number of epochs to train
  num_epochs: 30000
  # The learning rate for ADAM optimizer. 
  # LR scheduler info: https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling
  lr: 0.0001
  # Initial Learning Rate d_model^(-0.5)
  lr0: 0.044194174
  # The warmup steps for the learning rate scheduler
  warmup_steps: 4000
  # Beta1 for ADAM optimizer
  beta1: 0.9
  # Beta2 for ADAM optimizer
  beta2: 0.98
  # Folder name to save the model weights
  model_folder: "weights"
  # Basename for the model weights
  model_basename: "tmodel_"
  # The model to preload
  preload: "latest"
  # Tokenizer template: The {0} will be replaced by the language code
  tokenizer_file: "tokenizer_{0}.json"
  # The number of samples to use for validation
  num_val_samples: 3
  # Interval to save the model weights in epochs
  save_interval: 20

tensorboard:
  # The folder to save the logs for Tensorboard
  log_dir: "runs/tmodel"
